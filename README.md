# ğŸ¥ YouTube Analytics Pipeline

> **Automated, productionâ€‘ready ETL pipeline for YouTube channel analytics powered by Apache Airflow ğŸš€**

---

## âœ¨ What is this?

This project builds a **fully automated ETL pipeline** that pulls analytics data from the **YouTube Data API**, transforms it into meaningful metrics, and stores it reliably in **PostgreSQL** â€” all orchestrated by **Apache Airflow** and running smoothly inside **Docker**.

Perfect for:

* ğŸ“Š Tracking channel growth
* ğŸ“ˆ Monitoring performance trends
* ğŸ§ª Practicing realâ€‘world data engineering
* âš™ï¸ Learning Airflow + Docker the right way

---

## âš¡ Quick Start

### ğŸ”‘ Prerequisites

Make sure you have the following installed:

* ğŸ³ Docker & Docker Compose
* ğŸ” YouTube API Key
* ğŸ“º YouTube Channel ID

---

### ğŸ›  Installation

```bash
# Clone the repository
git clone <repository-url>
cd youtube_analytics_pipeline

# Setup environment variables
cp .env.example .env
nano .env

# Initialize Airflow
docker-compose up airflow-init

# Start all services
docker-compose up -d

# Verify running containers
docker-compose ps
```

---

## ğŸŒ Access Airflow UI

Once everything is running, open Airflow in your browser:

* ğŸ”— **URL**: [http://localhost:8080](http://localhost:8080)
* ğŸ‘¤ **Username**: `admin`
* ğŸ”‘ **Password**: `admin`

âœ¨ Youâ€™re now ready to manage pipelines!

---

## ğŸ“ Project Structure

```text
youtube_analytics_pipeline/
â”œâ”€â”€ dags/                 # ğŸ›« Airflow DAG definitions
â”œâ”€â”€ plugins/              # ğŸ”Œ Custom operators & hooks
â”œâ”€â”€ scripts/              # ğŸ§  SQL scripts & configs
â”œâ”€â”€ tests/                # ğŸ§ª Unit & integration tests
â”œâ”€â”€ docker-compose.yml    # ğŸ³ Docker services
â”œâ”€â”€ .env                  # ğŸ” Environment variables
â”œâ”€â”€ requirements.txt      # ğŸ“¦ Python dependencies
â””â”€â”€ README.md             # ğŸ“˜ Documentation
```

---

## ğŸ”§ Configuration

### ğŸ“„ Environment Variables (`.env`)

```env
YOUTUBE_API_KEY=your_api_key_here
LAYMAN_AI_CHANNEL_ID=your_channel_id_here
AIRFLOW_UID=50000
```

ğŸ”’ **Tip:** Never commit your real API keys to GitHub.

---

## â° DAG Schedules

| Pipeline         | Schedule    | Description                  |
| ---------------- | ----------- | ---------------------------- |
| ğŸ“Š Main ETL      | `0 6 * * *` | Daily data extraction & load |
| âœ… Quality Checks | `0 7 * * *` | Validate data accuracy       |
| ğŸ’¾ Backup        | `0 0 * * 0` | Weekly database backup       |

---

## ğŸ”„ Pipeline Flow

```text
YouTube API
    â†“
Extract â”€â”€â–º Transform â”€â”€â–º Load
    â†“            â†“          â†“
 Logging     Metrics     PostgreSQL
```

### ğŸ§© Steps Explained

1. **Extract** ğŸ“¡
   Fetch raw analytics data using the YouTube Data API

2. **Transform** ğŸ§¹
   Clean data, compute KPIs, and prepare structured tables

3. **Load** ğŸ—„ï¸
   Store processed data in PostgreSQL

4. **Monitor** ğŸš¨
   Run quality checks and alert on failures

---

## ğŸ Debug & Maintenance

Useful commands while developing or debugging:

```bash
# View service logs
docker-compose logs -f

# Access PostgreSQL
docker-compose exec postgres psql -U airflow -d airflow

# Trigger DAG manually
docker-compose exec airflow-webserver airflow dags trigger youtube_analytics_etl
```

---

## ğŸ“ Need Help?

If something isnâ€™t working:

1. ğŸ” Check logs: `docker-compose logs`
2. ğŸ§ª Verify `.env` values
3. ğŸŒ Test YouTube API connectivity
4. ğŸ”„ Restart services if needed

---

## ğŸ’¡ Future Enhancements

* ğŸ“ˆ Add dashboard (Metabase / Superset)
* â˜ï¸ Cloud deployment (AWS / GCP)
* ğŸ§  Incremental loads
* ğŸ”” Slack / Email alerts

---

### â¤ï¸ Built for learning, scaling, and realâ€‘world data engineering

Happy piping! ğŸš€
